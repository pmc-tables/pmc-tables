{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.getenv('SLURM_TMPDIR'):\n",
    "    SPARK_TMPDIR = Path(os.getenv('SLURM_TMPDIR')).resolve(strict=True)\n",
    "elif os.getenv(\"TMPDIR\"):\n",
    "    SPARK_TMPDIR = Path(os.getenv('TMPDIR'))\n",
    "elif os.getenv('SCRATCH'):\n",
    "    SPARK_TMPDIR = Path(os.getenv('SCRATCH')).joinpath('tmp')\n",
    "else:\n",
    "    raise Exception(\"Could not find a temporary directory for SPARK data!\")\n",
    "    \n",
    "SPARK_TMPDIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "vmem = psutil.virtual_memory().total // 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "if \"SPARK_MASTER_HOST\" in os.environ:\n",
    "    SPARK_MASTER = f\"spark://{os.environ['SPARK_MASTER_HOST']}:7077\"\n",
    "\n",
    "    CORES_PER_WORKER = 16\n",
    "    num_workers = max(1, psutil.cpu_count() // CORES_PER_WORKER)\n",
    "    print(f\"num_workers: {num_workers}\")\n",
    "    # Make sure we are not wasting any cores\n",
    "    if num_workers != psutil.cpu_count() / CORES_PER_WORKER:\n",
    "        print(\"WARNING!!! Not using all available CPUs!\")\n",
    "\n",
    "    spark_conf.set(\"spark.driver.memory\", \"65000M\")\n",
    "    spark_conf.set(\"spark.driver.maxResultSize\", \"65000M\")\n",
    "\n",
    "    spark_conf.set(\"spark.executor.cores\", f\"{CORES_PER_WORKER}\")\n",
    "    spark_conf.set(\"spark.executor.memory\", f\"{int((vmem - 1024) * 0.8 / num_workers)}M\")\n",
    "\n",
    "    spark_conf.set(\"spark.network.timeout\", \"600s\")\n",
    "    spark_conf.set(\"spark.sql.shuffle.partitions\", \"2001\")\n",
    "\n",
    "    # spark_conf.set(\"spark.local.dirs\", SPARK_TMPDIR.as_posix())\n",
    "else: \n",
    "    SPARK_MASTER = f\"local[{psutil.cpu_count()}]\"\n",
    "\n",
    "    driver_memory = min(64000, int(vmem // 2))\n",
    "    executor_memory = int(vmem - driver_memory)\n",
    "\n",
    "    spark_conf.set(\"spark.driver.memory\", f\"{driver_memory}M\")\n",
    "    spark_conf.set(\"spark.driver.maxResultSize\", f\"{driver_memory}M\")\n",
    "\n",
    "    spark_conf.set(\"spark.executor.memory\", f\"{executor_memory}M\")\n",
    "\n",
    "    # spark_conf.set(\"spark.network.timeout\", \"600s\")\n",
    "    spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "    spark_conf.set(\"spark.local.dirs\", SPARK_TMPDIR.as_posix())\n",
    "    spark_conf.set(\"spark.driver.extraJavaOptions\", f\"-Djava.io.tmpdir={SPARK_TMPDIR.as_posix()}\")\n",
    "    spark_conf.set(\"spark.executor.extraJavaOptions\", f\"-Djava.io.tmpdir={SPARK_TMPDIR.as_posix()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    SPARK_CONF_EXTRA\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    for key, value in SPARK_CONF_EXTRA.items():\n",
    "        spark_conf.set(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(SPARK_MASTER)\n",
    "    .appName(op.basename(op.dirname(os.getcwd())))\n",
    "    .config(conf=spark_conf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert spark.conf.get(\"spark.sql.execution.arrow.enabled\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(spark)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
